{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ YOLO Custom Training Workflow â€” Complete Guide\n",
    "\n",
    "This notebook walks you through the **entire process** of training a custom YOLOv8 model:\n",
    "\n",
    "1. **Setup** â€” Create dataset folder structure\n",
    "2. **Collect** â€” Capture images from your webcam\n",
    "3. **Annotate** â€” Label images using our web-based Annotator tool\n",
    "4. **Split** â€” Divide into training and validation sets\n",
    "5. **Train** â€” Train YOLOv8 on your custom data\n",
    "6. **Evaluate** â€” Check how well the model performs\n",
    "7. **Predict** â€” Test on new images or webcam\n",
    "8. **Export** â€” Convert to other formats for deployment\n",
    "\n",
    "**No Roboflow, no cloud, no internet needed** (after initial setup).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- `ultralytics` installed (`uv sync` from the project root)\n",
    "- A webcam (optional, for capturing training images)\n",
    "- At least 50 images of the object(s) you want to detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ultralytics is installed\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything we need\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Set up paths\n",
    "# Change DATASET_NAME to whatever you're detecting\n",
    "DATASET_NAME = \"my_dataset\"\n",
    "BASE_DIR = Path(\".\").resolve()  # YoloExamples/ directory\n",
    "DATASET_DIR = BASE_DIR / DATASET_NAME\n",
    "\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Define Your Classes\n",
    "\n",
    "What objects do you want to detect? List them here.\n",
    "\n",
    "**Examples:**\n",
    "- Safety equipment: `[\"helmet\", \"no_helmet\"]`\n",
    "- Fruits: `[\"apple\", \"banana\", \"orange\"]`\n",
    "- Custom parts: `[\"screw\", \"bolt\", \"washer\"]`\n",
    "- Pets: `[\"my_cat\", \"my_dog\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  CHANGE THIS to your object classes!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CLASSES = [\"helmet\", \"no_helmet\"]\n",
    "\n",
    "print(f\"Number of classes: {len(CLASSES)}\")\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    print(f\"  Class {i}: {cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Dataset Folder Structure\n",
    "\n",
    "YOLO expects this exact structure:\n",
    "```\n",
    "my_dataset/\n",
    "â”œâ”€â”€ data.yaml          <- Config file (auto-generated below)\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ images/        <- 80% of your images\n",
    "â”‚   â””â”€â”€ labels/        <- Annotation .txt files\n",
    "â””â”€â”€ val/\n",
    "    â”œâ”€â”€ images/        <- 20% of your images\n",
    "    â””â”€â”€ labels/        <- Annotation .txt files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all directories\n",
    "for split in [\"train\", \"val\"]:\n",
    "    for folder in [\"images\", \"labels\"]:\n",
    "        path = DATASET_DIR / split / folder\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  Created: {path}\")\n",
    "\n",
    "# Create data.yaml configuration file\n",
    "data_yaml_path = DATASET_DIR / \"data.yaml\"\n",
    "data_yaml_content = f\"\"\"# YOLOv8 Custom Dataset Configuration\n",
    "# Auto-generated by yolo_training_workflow.ipynb\n",
    "\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: {len(CLASSES)}\n",
    "names: {CLASSES}\n",
    "\"\"\"\n",
    "\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(f\"\\nCreated: {data_yaml_path}\")\n",
    "print(f\"\\ndata.yaml contents:\")\n",
    "print(data_yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Capture Training Images (Optional)\n",
    "\n",
    "If you already have images, skip this step and copy them into\n",
    "`my_dataset/train/images/` and `my_dataset/val/images/`.\n",
    "\n",
    "Otherwise, use the cell below to capture images from your webcam.\n",
    "\n",
    "**Tips for good training images:**\n",
    "- Capture from **different angles** and **distances**\n",
    "- Include **different lighting** conditions\n",
    "- Include **different backgrounds**\n",
    "- Aim for **50-500 images** per class\n",
    "- More variety = better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_images(save_dir, num_images=50, delay_ms=500):\n",
    "    \"\"\"\n",
    "    Capture images from webcam for training data.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory to save captured images\n",
    "        num_images: Number of images to capture\n",
    "        delay_ms: Delay between captures in milliseconds\n",
    "    \n",
    "    Controls:\n",
    "        SPACE  - Capture an image manually\n",
    "        a      - Toggle auto-capture mode\n",
    "        q      - Quit capturing\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Count existing images to continue numbering\n",
    "    existing = glob.glob(os.path.join(save_dir, \"*.jpg\"))\n",
    "    count = len(existing)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ERROR: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    auto_capture = False\n",
    "    print(f\"Webcam opened. Saving to: {save_dir}\")\n",
    "    print(f\"Already have {count} images.\")\n",
    "    print(\"SPACE=capture, a=auto-capture, q=quit\")\n",
    "    \n",
    "    while count < num_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Show info on frame\n",
    "        info = f\"Captured: {count}/{num_images}\"\n",
    "        mode = \" [AUTO]\" if auto_capture else \" [MANUAL]\"\n",
    "        cv2.putText(frame, info + mode, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "                    (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Capture Training Images\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(delay_ms if auto_capture else 30)\n",
    "        key = key & 0xFF\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' ') or auto_capture:\n",
    "            # Save the frame\n",
    "            filename = f\"img_{count:04d}.jpg\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            cv2.imwrite(filepath, frame)\n",
    "            count += 1\n",
    "            if not auto_capture:\n",
    "                print(f\"  Saved: {filename}\")\n",
    "        elif key == ord('a'):\n",
    "            auto_capture = not auto_capture\n",
    "            state = \"ON\" if auto_capture else \"OFF\"\n",
    "            print(f\"  Auto-capture: {state}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nDone! Captured {count} images in {save_dir}\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Uncomment the line below to start capturing!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# capture_images(str(DATASET_DIR / \"all_images\"), num_images=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Annotate Images\n",
    "\n",
    "Now we need to draw bounding boxes around objects in each image.\n",
    "\n",
    "Use our **web-based Annotator tool** â€” no internet or external services needed!\n",
    "\n",
    "### How to annotate:\n",
    "\n",
    "1. Open a **separate terminal** and run:\n",
    "```bash\n",
    "uv run python YoloExamples/annotate/app.py\n",
    "```\n",
    "\n",
    "2. Open **http://localhost:5000** in your browser\n",
    "\n",
    "3. Click **\"Select Folder\"** and navigate to your `train/images/` folder\n",
    "\n",
    "4. Add your classes (e.g., `helmet`, `no_helmet`)\n",
    "\n",
    "5. Draw bounding boxes on each image:\n",
    "   - Select a class â†’ click & drag to draw a box\n",
    "   - Switch class â†’ draw boxes for different objects\n",
    "   - **Multiple objects per image**: each box becomes a line in the `.txt` label file\n",
    "\n",
    "6. Repeat for `val/images/`\n",
    "\n",
    "### Multi-class labeling example:\n",
    "\n",
    "If an image has a person WITH a helmet and another WITHOUT:\n",
    "```\n",
    "0 0.4807 0.2089 0.3461 0.3750    â† class 0 (helmet)\n",
    "1 0.7200 0.5100 0.2000 0.3000    â† class 1 (no_helmet)\n",
    "```\n",
    "Each line = one bounding box. Different lines can have different class IDs.\n",
    "\n",
    "### Keyboard shortcuts (in the Annotator):\n",
    "| Key | Action |\n",
    "|-----|--------|\n",
    "| `1-9` | Select class |\n",
    "| `D` / `â†’` | Next image (auto-saves) |\n",
    "| `A` / `â†` | Previous image |\n",
    "| `S` | Save labels |\n",
    "| `Z` | Undo last box |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Annotation is done via the web-based Annotator tool.\n",
    "#  Run in a separate terminal:\n",
    "#    uv run python YoloExamples/annotate/app.py\n",
    "#  Then open http://localhost:5000 in your browser.\n",
    "#\n",
    "#  Point it to your train/images/ and val/images/ folders\n",
    "#  to draw bounding boxes and save YOLO-format labels.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"Annotate your images using the web Annotator tool:\")\n",
    "print(\"  uv run python YoloExamples/annotate/app.py\")\n",
    "print(\"  Then open http://localhost:5000\")\n",
    "print()\n",
    "print(\"Once done, come back here and continue with Step 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Auto-Split Images (Optional)\n",
    "\n",
    "If you have all your images in one folder (e.g., `all_images/`),\n",
    "this cell will automatically split them 80/20 into train/val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_split_dataset(source_dir, dataset_dir, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split images + labels from source_dir into train/val.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Folder with images (and optionally labels)\n",
    "        dataset_dir: Target dataset directory\n",
    "        ratio: Fraction for training (0.8 = 80% train, 20% val)\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n",
    "    images = [\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if os.path.splitext(f)[1].lower() in exts\n",
    "    ]\n",
    "    \n",
    "    if not images:\n",
    "        print(f\"No images found in {source_dir}\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(images)\n",
    "    split = int(len(images) * ratio)\n",
    "    train_imgs = images[:split]\n",
    "    val_imgs = images[split:]\n",
    "    \n",
    "    print(f\"Total: {len(images)} | \"\n",
    "          f\"Train: {len(train_imgs)} | Val: {len(val_imgs)}\")\n",
    "    \n",
    "    for split_name, img_list in [(\"train\", train_imgs),\n",
    "                                  (\"val\", val_imgs)]:\n",
    "        img_dir = os.path.join(dataset_dir, split_name, \"images\")\n",
    "        lbl_dir = os.path.join(dataset_dir, split_name, \"labels\")\n",
    "        os.makedirs(img_dir, exist_ok=True)\n",
    "        os.makedirs(lbl_dir, exist_ok=True)\n",
    "        \n",
    "        for img_name in img_list:\n",
    "            # Copy image\n",
    "            src = os.path.join(source_dir, img_name)\n",
    "            shutil.copy2(src, img_dir)\n",
    "            \n",
    "            # Copy label if exists\n",
    "            base = os.path.splitext(img_name)[0]\n",
    "            lbl_src = os.path.join(source_dir, base + \".txt\")\n",
    "            if os.path.exists(lbl_src):\n",
    "                shutil.copy2(lbl_src, lbl_dir)\n",
    "    \n",
    "    print(\"Split complete!\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Uncomment to auto-split from an 'all_images' folder:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# auto_split_dataset(\n",
    "#     str(DATASET_DIR / \"all_images\"),\n",
    "#     str(DATASET_DIR),\n",
    "#     ratio=0.8\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Verify Dataset\n",
    "\n",
    "Before training, let's check that everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images and labels\n",
    "for split in [\"train\", \"val\"]:\n",
    "    img_dir = DATASET_DIR / split / \"images\"\n",
    "    lbl_dir = DATASET_DIR / split / \"labels\"\n",
    "    \n",
    "    imgs = list(img_dir.glob(\"*\")) if img_dir.exists() else []\n",
    "    lbls = list(lbl_dir.glob(\"*.txt\")) if lbl_dir.exists() else []\n",
    "    \n",
    "    # Filter to only image files\n",
    "    img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    imgs = [i for i in imgs if i.suffix.lower() in img_exts]\n",
    "    \n",
    "    print(f\"{split:>5}: {len(imgs)} images, {len(lbls)} labels\")\n",
    "    \n",
    "    if len(imgs) != len(lbls):\n",
    "        print(f\"  âš  WARNING: Image/label count mismatch!\")\n",
    "        # Find unlabeled images\n",
    "        labeled = {l.stem for l in lbls}\n",
    "        unlabeled = [i.name for i in imgs if i.stem not in labeled]\n",
    "        if unlabeled:\n",
    "            print(f\"  Unlabeled: {unlabeled[:5]}...\")\n",
    "\n",
    "print(f\"\\ndata.yaml: {data_yaml_path}\")\n",
    "if data_yaml_path.exists():\n",
    "    print(data_yaml_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few annotated images to verify labels are correct\n",
    "def show_annotated_samples(image_dir, label_dir, classes,\n",
    "                           num_samples=4):\n",
    "    \"\"\"Display annotated images with bounding boxes.\"\"\"\n",
    "    img_exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "    images = [\n",
    "        f for f in os.listdir(image_dir)\n",
    "        if os.path.splitext(f)[1].lower() in img_exts\n",
    "    ]\n",
    "    \n",
    "    if not images:\n",
    "        print(\"No images found.\")\n",
    "        return\n",
    "    \n",
    "    samples = images[:num_samples]\n",
    "    fig, axes = plt.subplots(1, len(samples),\n",
    "                             figsize=(5 * len(samples), 5))\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors_rgb = [\n",
    "        (0, 1, 0), (1, 0, 0), (0, 0, 1), (1, 1, 0),\n",
    "        (0, 1, 1), (1, 0, 1)\n",
    "    ]\n",
    "    \n",
    "    for ax, img_name in zip(axes, samples):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(img_name, fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Draw labels\n",
    "        base = os.path.splitext(img_name)[0]\n",
    "        lbl_path = os.path.join(label_dir, base + \".txt\")\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    cid = int(parts[0])\n",
    "                    cx = float(parts[1]) * w\n",
    "                    cy = float(parts[2]) * h\n",
    "                    bw = float(parts[3]) * w\n",
    "                    bh = float(parts[4]) * h\n",
    "                    x1 = cx - bw / 2\n",
    "                    y1 = cy - bh / 2\n",
    "                    \n",
    "                    color = colors_rgb[cid % len(colors_rgb)]\n",
    "                    name = classes[cid] if cid < len(classes) else str(cid)\n",
    "                    \n",
    "                    rect = plt.Rectangle(\n",
    "                        (x1, y1), bw, bh,\n",
    "                        linewidth=2, edgecolor=color,\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(x1, y1 - 5, name, color=color,\n",
    "                            fontsize=8, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Uncomment to visualize training samples:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# show_annotated_samples(\n",
    "#     str(DATASET_DIR / \"train\" / \"images\"),\n",
    "#     str(DATASET_DIR / \"train\" / \"labels\"),\n",
    "#     CLASSES\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Train the Model!\n",
    "\n",
    "This is where the magic happens. We start from the pre-trained\n",
    "`yolov8n.pt` (which already knows general features like edges and\n",
    "shapes) and fine-tune it on YOUR specific objects.\n",
    "\n",
    "**Training time depends on:**\n",
    "- Number of images (more = longer)\n",
    "- Number of epochs (more = longer but potentially better)\n",
    "- GPU vs CPU (GPU is 10-50x faster)\n",
    "- Model size (nano is fastest)\n",
    "\n",
    "**Rough estimates for 100 images, 50 epochs:**\n",
    "- GPU (NVIDIA): ~5-15 minutes\n",
    "- CPU (laptop): ~30-90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Training Configuration â€” Adjust these!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EPOCHS = 50          # Training rounds (50-100 for small datasets)\n",
    "IMAGE_SIZE = 640     # Input image size\n",
    "BATCH_SIZE = 16      # Reduce to 8 or 4 if you get memory errors\n",
    "BASE_MODEL = \"yolov8n.pt\"  # Nano model (fastest)\n",
    "\n",
    "print(f\"Model:      {BASE_MODEL}\")\n",
    "print(f\"Dataset:    {data_yaml_path}\")\n",
    "print(f\"Epochs:     {EPOCHS}\")\n",
    "print(f\"Image size: {IMAGE_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = YOLO(BASE_MODEL)\n",
    "\n",
    "# Start training!\n",
    "# This will print progress for each epoch.\n",
    "results = model.train(\n",
    "    data=str(data_yaml_path),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    name=\"custom_train\",\n",
    "    project=str(BASE_DIR / \"runs\" / \"detect\"),\n",
    "    exist_ok=True,\n",
    "    # Data augmentation (helps small datasets generalize)\n",
    "    hsv_h=0.015,     # Hue variation\n",
    "    hsv_s=0.7,       # Saturation variation\n",
    "    hsv_v=0.4,       # Brightness variation\n",
    "    fliplr=0.5,      # Horizontal flip 50% of the time\n",
    "    mosaic=1.0,      # Mosaic augmentation (combines 4 images)\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Training Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Evaluate Training Results\n",
    "\n",
    "Let's see how well the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the best trained model\n",
    "BEST_MODEL = BASE_DIR / \"runs\" / \"detect\" / \"custom_train\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "if BEST_MODEL.exists():\n",
    "    print(f\"Best model: {BEST_MODEL}\")\n",
    "    print(f\"Model size: {BEST_MODEL.stat().st_size / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"No trained model found. Run training first!\")\n",
    "    # Fall back to base model for demo\n",
    "    BEST_MODEL = Path(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curves\n",
    "results_img = (BASE_DIR / \"runs\" / \"detect\" / \"custom_train\" \n",
    "               / \"results.png\")\n",
    "\n",
    "if results_img.exists():\n",
    "    img = mpimg.imread(str(results_img))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Training Results', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results.png found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "conf_matrix = (BASE_DIR / \"runs\" / \"detect\" / \"custom_train\" \n",
    "               / \"confusion_matrix.png\")\n",
    "\n",
    "if conf_matrix.exists():\n",
    "    img = mpimg.imread(str(conf_matrix))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Confusion Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No confusion matrix found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation to get metrics\n",
    "if BEST_MODEL.exists() and data_yaml_path.exists():\n",
    "    val_model = YOLO(str(BEST_MODEL))\n",
    "    metrics = val_model.val(data=str(data_yaml_path))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"  Validation Metrics\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"  mAP50:     {metrics.box.map50:.4f}\")\n",
    "    print(f\"  mAP50-95:  {metrics.box.map:.4f}\")\n",
    "    print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
    "    print(f\"  Recall:    {metrics.box.mr:.4f}\")\n",
    "    print()\n",
    "    print(\"  What do these mean?\")\n",
    "    print(\"  - mAP50: How well the model detects objects\")\n",
    "    print(\"           (higher = better, max = 1.0)\")\n",
    "    print(\"  - Precision: Of all detections, how many are correct\")\n",
    "    print(\"  - Recall: Of all real objects, how many were found\")\n",
    "else:\n",
    "    print(\"Train a model first to see validation metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Test on New Images\n",
    "\n",
    "Let's see the trained model in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image\n",
    "def predict_and_show(model_path, image_path):\n",
    "    \"\"\"Run prediction and display the result.\"\"\"\n",
    "    model = YOLO(str(model_path))\n",
    "    results = model(image_path, conf=0.5)\n",
    "    \n",
    "    # Get the annotated image\n",
    "    annotated = results[0].plot()  # BGR numpy array\n",
    "    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    ax.imshow(annotated_rgb)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Detections: {len(results[0].boxes)}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detections\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        name = results[0].names[cls_id]\n",
    "        print(f\"  {name}: {conf:.2f}\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Uncomment and set the image path to test:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# predict_and_show(BEST_MODEL, \"path/to/test_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on webcam (opens an OpenCV window)\n",
    "def predict_webcam(model_path):\n",
    "    \"\"\"Run the trained model on webcam feed.\"\"\"\n",
    "    model = YOLO(str(model_path))\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"Webcam running. Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        results = model(frame, conf=0.5, verbose=False)\n",
    "        annotated = results[0].plot()\n",
    "        \n",
    "        cv2.imshow(\"Custom Model - Webcam\", annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Uncomment to test on webcam:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# predict_webcam(BEST_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Export for Deployment\n",
    "\n",
    "Export your trained model to different formats for deployment\n",
    "on various platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (universal, works on any platform)\n",
    "if BEST_MODEL.exists():\n",
    "    export_model = YOLO(str(BEST_MODEL))\n",
    "    \n",
    "    # ONNX â€” Universal format\n",
    "    # export_model.export(format=\"onnx\")\n",
    "    \n",
    "    # TensorRT â€” NVIDIA GPUs (fastest)\n",
    "    # export_model.export(format=\"engine\")\n",
    "    \n",
    "    # TFLite â€” Mobile / Raspberry Pi\n",
    "    # export_model.export(format=\"tflite\")\n",
    "    \n",
    "    # OpenVINO â€” Intel devices\n",
    "    # export_model.export(format=\"openvino\")\n",
    "    \n",
    "    print(\"Uncomment the export format you need above.\")\n",
    "    print(\"Available formats: onnx, engine, tflite, coreml, openvino\")\n",
    "else:\n",
    "    print(\"Train a model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: The Complete Workflow\n",
    "\n",
    "```\n",
    "1. Define classes          â†’ CLASSES = [\"helmet\", \"no_helmet\"]\n",
    "2. Create folder structure â†’ my_dataset/train/images, etc.\n",
    "3. Capture/collect images  â†’ Webcam or existing photos\n",
    "4. Annotate locally        â†’ uv run python YoloExamples/annotate/app.py\n",
    "5. Split train/val         â†’ 80/20 split\n",
    "6. Train                   â†’ model.train(data=..., epochs=50)\n",
    "7. Evaluate                â†’ Check mAP, precision, recall\n",
    "8. Test                    â†’ Run on new images or webcam\n",
    "9. Export                  â†’ ONNX, TensorRT, TFLite, etc.\n",
    "```\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "- **More images = better model** (aim for 100+ per class)\n",
    "- **Diverse images** â€” vary angle, lighting, background, distance\n",
    "- **Accurate labels** â€” take time to draw tight bounding boxes\n",
    "- **More epochs** â€” try 100 if 50 isn't enough (watch for overfitting)\n",
    "- **Larger model** â€” try `yolov8s.pt` if nano isn't accurate enough\n",
    "- **Data augmentation** â€” already enabled by default in our config\n",
    "\n",
    "### Multi-Class Labeling\n",
    "\n",
    "Each image's `.txt` label file can have **multiple lines** with **different class IDs**:\n",
    "```\n",
    "0 0.4807 0.2089 0.3461 0.3750    â† class 0 (helmet)\n",
    "1 0.7200 0.5100 0.2000 0.3000    â† class 1 (no_helmet)\n",
    "```\n",
    "The Annotator tool handles this automatically â€” just switch classes and draw boxes.\n",
    "\n",
    "### Connecting to ESP8266\n",
    "\n",
    "Once your custom model works, you can use it to control hardware!\n",
    "See `object_detection.py` and the workshop docs for how to send\n",
    "UDP commands to the ESP8266 based on detections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
